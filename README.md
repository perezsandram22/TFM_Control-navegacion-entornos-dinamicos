# üöó Aprendizaje por Refuerzo para Control de Veh√≠culo en Webots

Este proyecto implementa un agente de **Inteligencia Artificial** que aprende a controlar un veh√≠culo en el simulador **Webots** mediante t√©cnicas de **aprendizaje por refuerzo**. Utiliza las bibliotecas **Gymnasium** y **Stable Baselines3**, comunic√°ndose con Webots a trav√©s de **sockets TCP**.

---

## üìö Tabla de Contenido

- [üîß Componentes del Proyecto](#-componentes-del-proyecto)
- [‚öôÔ∏è Requisitos Previos](#Ô∏è-requisitos-previos)
- [üì¶ Instalaci√≥n](#-instalaci√≥n)
- [üß™ Configuraci√≥n de Webots](#-configuraci√≥n-de-webots)
- [üèãÔ∏è‚Äç‚ôÇÔ∏è Entrenamiento del Agente](#Ô∏è-entrenamiento-del-agente)
- [üéØ Evaluaci√≥n del Modelo](#-evaluaci√≥n-del-modelo)
- [üß† Estructura de Observaci√≥n y Acci√≥n](#-estructura-de-observaci√≥n-y-acci√≥n)
- [üìå Notas Finales](#-notas-finales)

---

## üîß Componentes del Proyecto

| Archivo                | Descripci√≥n |
|------------------------|-------------|
| `webots_car_env.py`    | Define el entorno Gymnasium que conecta el agente con Webots. Incluye soporte opcional para observaciones extendidas con MPC. |
| `train_webots_car.py`  | Script principal para entrenar el modelo PPO. Exporta resultados a `entrenamiento_webots.xlsx`. |
| `train_car_MPC.py`     | Variante con MPC. Usa `MPCLoggingCallback` y exporta a `training_mpc_logs.xlsx`. |
| `test_webots_car.py`   | Carga y prueba un modelo PPO entrenado en Webots. |
| `requirements.txt`     | Lista de dependencias necesarias para ejecutar el proyecto. |

---

## ‚öôÔ∏è Requisitos Previos

- Tener instalado **Webots**.
- Configurar un mundo con veh√≠culo y controlador que se comunique por **socket TCP** en el **puerto 10000**.

---

## üì¶ Instalaci√≥n

Instala las dependencias necesarias con:

> [!NOTE]
> pip install -r requirements.txt

üß™ Configurar y Ejecutar Webots

- Inicia el simulador Webots.
- Abre el mundo virtual donde est√° el veh√≠culo que ser√° controlado.
- Aseg√∫rate de que el controlador del veh√≠culo en Webots est√© configurado para establecer una conexi√≥n de socket con el script de Python.

üèãÔ∏è‚Äç‚ôÇÔ∏è Entrenar al Agente

Para comenzar el entrenamiento, ejecuta el script principal:

> [!NOTE]
> python train_webots_car.py

O, si prefieres usar la versi√≥n con el callback de MPC:

> [!NOTE]
> python train_car_MPC.py

üìå El script imprimir√° el progreso en la consola. Al finalizar:

El modelo entrenado se guardar√° como ppo_webots_car.zip o ppo_webots_mpc.zip.

Se generar√° un archivo de Excel con los registros del entrenamiento.

üéØ Probar el Agente Entrenado

Una vez que tengas un modelo guardado, puedes probar su rendimiento ejecutando:

> [!NOTE]
> python test_webots_car.py

Este script cargar√° el modelo y lo ejecutar√° en el entorno de Webots de manera determinista, lo que te permitir√° ver c√≥mo el agente navega.

üß† Estructura de la Observaci√≥n y la Acci√≥n
üîç Espacio de Observaci√≥n
El agente recibe un vector que puede variar en tama√±o dependiendo de la configuraci√≥n del entorno (mpc_in_obs):

Observaci√≥n normal: [steering, min_lidar, bumper, velocidad, obstacle_direction]

Observaci√≥n extendida (con MPC): [steering, min_lidar, bumper, velocidad, obstacle_direction, mpc_steering, mpc_velocidad]

üéÆ Espacio de Acci√≥n
El agente controla dos par√°metros del veh√≠culo:

steering_value

velocity_value

üß† control.py ‚Äî Controlador Webots
Este script se ejecuta dentro del simulador Webots y act√∫a como puente entre el entorno simulado y el agente de aprendizaje por refuerzo. Sus principales funciones incluyen:

-Inicializaci√≥n de sensores y actuadores del veh√≠culo (GPS, Lidar, c√°mara, ruedas, direcci√≥n, bumper).
-Servidor TCP que espera comandos desde el entorno Gymnasium.
-Gesti√≥n de episodios: reinicio de posici√≥n, m√©tricas de desempe√±o, detecci√≥n de colisiones y obst√°culos.

üìå C√°lculo de recompensas basadas en:

-Desviaci√≥n del carril (usando visi√≥n por c√°mara).
-Proximidad a obst√°culos (Lidar).
-Colisiones reales (bumper).
-Cambios bruscos de direcci√≥n.
-Velocidad y eficiencia de trayectoria.

Comunicaci√≥n continua con el agente: recibe acciones y env√≠a observaciones, recompensas y estados de terminaci√≥n.

Este archivo debe estar configurado como controlador del veh√≠culo en Webots, y debe estar vinculado al nodo TESLA_MODEL3 en el mundo .wbt.

El proyecto desarrollado en webots est√° organizado de la siguiente manera:

<img width="256" height="618" alt="Captura de pantalla 2025-09-15 a la(s) 5 04 11‚ÄØp m" src="https://github.com/user-attachments/assets/a2aa20de-b3b5-4902-beb6-f8c9b6e09e98" />

Esta estructura incluye:

* Controladores para veh√≠culos aut√≥nomos y sem√°foros.
* Plugins personalizados para visualizaci√≥n y control del autom√≥vil.
* Mundos simulados en Webots, como city.wbt.
* Archivos fuente y ejecutables para compilar y ejecutar los controladores.

üìå Notas Finales

Este proyecto es una excelente base para explorar el uso del aprendizaje por refuerzo en rob√≥tica simulada. Puedes extenderlo para incluir otros algoritmos, sensores o entornos m√°s complejos.
