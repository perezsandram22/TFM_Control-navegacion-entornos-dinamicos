# ğŸš— Aprendizaje por Refuerzo para Control de VehÃ­culo en Webots

Este proyecto implementa un agente de **Inteligencia Artificial** que aprende a controlar un vehÃ­culo en el simulador **Webots** mediante tÃ©cnicas de **aprendizaje por refuerzo**. Utiliza las bibliotecas **Gymnasium** y **Stable Baselines3**, comunicÃ¡ndose con Webots a travÃ©s de **sockets TCP**.

---

## ğŸ“š Tabla de Contenido

- [ğŸ”§ Componentes del Proyecto](#-componentes-del-proyecto)
- [âš™ï¸ Requisitos Previos](#ï¸-requisitos-previos)
- [ğŸ“¦ InstalaciÃ³n](#-instalaciÃ³n)
- [ğŸ§ª ConfiguraciÃ³n de Webots](#-configuraciÃ³n-de-webots)
- [ğŸ‹ï¸â€â™‚ï¸ Entrenamiento del Agente](#ï¸-entrenamiento-del-agente)
- [ğŸ¯ EvaluaciÃ³n del Modelo](#-evaluaciÃ³n-del-modelo)
- [ğŸ§  Estructura de ObservaciÃ³n y AcciÃ³n](#-estructura-de-observaciÃ³n-y-acciÃ³n)
- [ğŸ“Œ Notas Finales](#-notas-finales)

---

## ğŸ”§ Componentes del Proyecto

| Archivo                | DescripciÃ³n |
|------------------------|-------------|
| `webots_car_env.py`    | Define el entorno Gymnasium que conecta el agente con Webots. Incluye soporte opcional para observaciones extendidas con MPC. |
| `train_webots_car.py`  | Script principal para entrenar el modelo PPO. Exporta resultados a `entrenamiento_webots.xlsx`. |
| `train_car_MPC.py`     | Variante con MPC. Usa `MPCLoggingCallback` y exporta a `training_mpc_logs.xlsx`. |
| `test_webots_car.py`   | Carga y prueba un modelo PPO entrenado en Webots. |
| `requirements.txt`     | Lista de dependencias necesarias para ejecutar el proyecto. |

---

## âš™ï¸ Requisitos Previos

- Tener instalado **Webots**.
- Configurar un mundo con vehÃ­culo y controlador que se comunique por **socket TCP** en el **puerto 10000**.

---

## ğŸ“¦ InstalaciÃ³n

Instala las dependencias necesarias con:

pip install -r requirements.txt

ğŸ§ª Configurar y Ejecutar Webots

- Inicia el simulador Webots.
- Abre el mundo virtual donde estÃ¡ el vehÃ­culo que serÃ¡ controlado.
- AsegÃºrate de que el controlador del vehÃ­culo en Webots estÃ© configurado para establecer una conexiÃ³n de socket con el script de Python.

ğŸ‹ï¸â€â™‚ï¸ Entrenar al Agente

Para comenzar el entrenamiento, ejecuta el script principal:

python train_webots_car.py

O, si prefieres usar la versiÃ³n con el callback de MPC:

python train_car_MPC.py

ğŸ“Œ El script imprimirÃ¡ el progreso en la consola. Al finalizar:

El modelo entrenado se guardarÃ¡ como ppo_webots_car.zip o ppo_webots_mpc.zip.

Se generarÃ¡ un archivo de Excel con los registros del entrenamiento.

ğŸ¯ Probar el Agente Entrenado

Una vez que tengas un modelo guardado, puedes probar su rendimiento ejecutando:

python test_webots_car.py

Este script cargarÃ¡ el modelo y lo ejecutarÃ¡ en el entorno de Webots de manera determinista, lo que te permitirÃ¡ ver cÃ³mo el agente navega.

ğŸ§  Estructura de la ObservaciÃ³n y la AcciÃ³n
ğŸ” Espacio de ObservaciÃ³n
El agente recibe un vector que puede variar en tamaÃ±o dependiendo de la configuraciÃ³n del entorno (mpc_in_obs):

ObservaciÃ³n normal: [steering, min_lidar, bumper, velocidad, obstacle_direction]

ObservaciÃ³n extendida (con MPC): [steering, min_lidar, bumper, velocidad, obstacle_direction, mpc_steering, mpc_velocidad]

ğŸ® Espacio de AcciÃ³n
El agente controla dos parÃ¡metros del vehÃ­culo:

steering_value

velocity_value

ğŸ§  control.py â€” Controlador Webots
Este script se ejecuta dentro del simulador Webots y actÃºa como puente entre el entorno simulado y el agente de aprendizaje por refuerzo. Sus principales funciones incluyen:

-InicializaciÃ³n de sensores y actuadores del vehÃ­culo (GPS, Lidar, cÃ¡mara, ruedas, direcciÃ³n, bumper).
-Servidor TCP que espera comandos desde el entorno Gymnasium.
-GestiÃ³n de episodios: reinicio de posiciÃ³n, mÃ©tricas de desempeÃ±o, detecciÃ³n de colisiones y obstÃ¡culos.

ğŸ“Œ CÃ¡lculo de recompensas basadas en:

-DesviaciÃ³n del carril (usando visiÃ³n por cÃ¡mara).
-Proximidad a obstÃ¡culos (Lidar).
-Colisiones reales (bumper).
-Cambios bruscos de direcciÃ³n.
-Velocidad y eficiencia de trayectoria.

ComunicaciÃ³n continua con el agente: recibe acciones y envÃ­a observaciones, recompensas y estados de terminaciÃ³n.

Este archivo debe estar configurado como controlador del vehÃ­culo en Webots, y debe estar vinculado al nodo TESLA_MODEL3 en el mundo .wbt.

ğŸ“Œ Notas Finales

Este proyecto es una excelente base para explorar el uso del aprendizaje por refuerzo en robÃ³tica simulada. Puedes extenderlo para incluir otros algoritmos, sensores o entornos mÃ¡s complejos.